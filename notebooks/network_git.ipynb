{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "collapsed": true,
        "id": "0OtK92W-wpbe",
        "outputId": "05f164ef-c963-4ac0-ad7e-40e5dbc50570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Leak-Project'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 25 (delta 3), reused 25 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (25/25), 1.01 MiB | 36.86 MiB/s, done.\n",
            "Resolving deltas: 100% (3/3), done.\n",
            "[Errno 2] No such file or directory: 'leak-project'\n",
            "/content\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mMounted at /content/drive\n",
            "cp: cannot create regular file 'config/local.yaml': No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pipe'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2977279690.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cp /content/drive/MyDrive/leak-project/config/local.yaml config/local.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"paths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pipe'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!git clone https://github.com/Jkoeppens/Leak-Project\n",
        "%cd leak-project\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Deine persönliche local.yaml in Drive ablegen:\n",
        "local_yaml_drive = \"/content/drive/MyDrive/leak-project/config/local.yaml\"\n",
        "import os, textwrap, pathlib\n",
        "pathlib.Path(local_yaml_drive).parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(local_yaml_drive, \"w\") as f:\n",
        "    f.write(textwrap.dedent(\"\"\"\\\n",
        "    paths:\n",
        "      root: \"/content/drive/MyDrive/leak-project\"\n",
        "    \"\"\"))\n",
        "\n",
        "# Link/Copy (einfachste Variante): local.yaml ins Repo-config spiegeln\n",
        "!cp /content/drive/MyDrive/leak-project/config/local.yaml config/local.yaml\n",
        "\n",
        "from pipe.config import load_config\n",
        "cfg = load_config()\n",
        "cfg[\"paths\"], cfg[\"outputs\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pipe.config import load_config\n",
        "from pipe.io import load_edges_prepared\n",
        "from pipe.diagnostics import outside_world_stats, save_outside_world_report\n",
        "\n",
        "cfg = load_config()\n",
        "E = load_edges_prepared(cfg)\n",
        "\n",
        "stats = outside_world_stats(E, ow_label=\"Outside world\")\n",
        "print(stats)  # früheres print(...) ersetzt\n",
        "\n",
        "# optional persistieren:\n",
        "saved = save_outside_world_report(stats, cfg)\n",
        "print(\"Report gespeichert:\", saved)\n"
      ],
      "metadata": {
        "id": "89pCyM2cyisn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pipe.config import load_config\n",
        "from pipe.prep import prepare_edges_and_report\n",
        "\n",
        "cfg = load_config()\n",
        "E_prepared, prep_path, rep = prepare_edges_and_report(cfg)\n",
        "print(\"[OK] prepared:\", prep_path)\n",
        "print(rep)\n"
      ],
      "metadata": {
        "id": "baE7lPhezV79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pipe.config import load_config\n",
        "from pipe.io import load_levels\n",
        "from pipe.diagnose_levels import analyze_levels\n",
        "\n",
        "cfg = load_config()  # lädt default.yaml + optional local.yaml\n",
        "print(\"[INFO] Levels-CSV:\", cfg[\"paths\"][\"levels\"])\n",
        "\n",
        "H = load_levels(cfg[\"paths\"][\"levels\"])\n",
        "df_stats = analyze_levels(H, cfg[\"outputs\"][\"cutoffs_dir\"], plot_levels=(\"level_2\",\"level_3\"))\n",
        "print(\"[OK] Level-Diagnose →\", cfg[\"outputs\"][\"cutoffs_dir\"])\n",
        "df_stats.head()\n"
      ],
      "metadata": {
        "id": "hgBPu0KF1gp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pipe.config import load_config\n",
        "from pipe.dynamic_levels import build_dynamic_selection\n",
        "\n",
        "cfg = load_config()\n",
        "sel = build_dynamic_selection(cfg)\n",
        "print(\"[OK] selection_dynamic.json & selection_meta.json in:\", cfg[\"outputs\"][\"org_dir\"])\n",
        "\n",
        "# kurze Übersicht:\n",
        "for LV in sel[\"levels\"]:\n",
        "    m = sel[\"meta\"].get(LV, {})\n",
        "    if m:\n",
        "        print(f\"{LV}: min_size={m.get('min_size')}, coverage={m.get('coverage')}, kept={m.get('kept')}/{m.get('clusters_total')}\")\n",
        "    i = sel[\"levels\"].index(LV)\n",
        "    if i < len(sel[\"levels\"])-1:\n",
        "        child_lv = sel[\"levels\"][i+1]\n",
        "        cap = sel[\"meta\"].get(child_lv, {}).get(\"max_children_per_parent\")\n",
        "        if cap is not None:\n",
        "            print(f\"   children cap (for {LV}→{child_lv}): {cap}\")\n"
      ],
      "metadata": {
        "id": "HDCp4MOf176l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pipe.config import load_config\n",
        "from pipe.inspect_levels import inspect_levels_and_write\n",
        "\n",
        "cfg = load_config()\n",
        "df_summary, df_pc_over, df_sim = inspect_levels_and_write(cfg)\n",
        "\n",
        "print(\"[OK] geschrieben nach:\", cfg[\"outputs\"][\"cutoffs_dir\"])\n",
        "df_summary.head(), df_pc_over.head(), df_sim.head()\n"
      ],
      "metadata": {
        "id": "gpDsGGLs3RKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from pipe.config import load_config\n",
        "from pipe.io import load_levels, load_edges_prepared   # oder load_edges, je nach Workflow\n",
        "from pipe.leaders import compute_leaders, save_leaders, find_external_leaders\n",
        "\n",
        "cfg = load_config()\n",
        "\n",
        "# Daten laden\n",
        "H = load_levels(cfg[\"paths\"][\"levels\"])\n",
        "E = pd.read_csv(cfg[\"paths\"][\"edges_prepared\"]).astype({\"src\": str, \"dst\": str, \"weight\": float})\n",
        "\n",
        "# Selection (aus D)\n",
        "sel_path = Path(cfg[\"outputs\"][\"org_dir\"]) / \"selection_dynamic.json\"\n",
        "selection = json.loads(sel_path.read_text())\n",
        "\n",
        "# Policy/Parameter\n",
        "iconf = cfg.get(\"filters\", {}).get(\"internal\", {})\n",
        "internal_mode = (iconf.get(\"mode\", \"outside_supernode\") or \"outside_supernode\").lower()\n",
        "internal_domains = tuple(iconf.get(\"domains\", [\"@enron.com\"]))\n",
        "allow_external = bool(iconf.get(\"allow_external_leaders\", False))\n",
        "\n",
        "pct_exec = float(cfg.get(\"thresholds\", {}).get(\"pct_exec\", 99.9))\n",
        "bt_cap   = int(cfg.get(\"runtime\", {}).get(\"bt_sample_cap\", 2000))\n",
        "seed     = int(cfg.get(\"runtime\", {}).get(\"random_seed\", 42))\n",
        "\n",
        "# Berechnen\n",
        "leaders = compute_leaders(\n",
        "    H, E, selection,\n",
        "    pct_exec=pct_exec, bt_cap=bt_cap, seed=seed,\n",
        "    internal_mode=internal_mode, internal_domains=internal_domains,\n",
        "    allow_external_leaders=allow_external\n",
        ")\n",
        "\n",
        "# Speichern & QA\n",
        "j_path, csv_path = save_leaders(leaders, cfg)\n",
        "print(\"[OK] leaders.json →\", j_path)\n",
        "print(\"[OK] leaders_flat.csv →\", csv_path)\n",
        "\n",
        "bad = find_external_leaders(leaders, internal_domains)\n",
        "print(\"Execs (n):\", len(leaders[\"execs\"]))\n",
        "for lv in selection[\"levels\"]:\n",
        "    print(f\"{lv}: {len(leaders['levels'][lv])} Leader-Einträge\")\n",
        "print(\"Externe Leader gefunden:\", len(bad))\n",
        "if bad[:5]:\n",
        "    print(\"Beispiele:\", bad[:5])\n"
      ],
      "metadata": {
        "id": "dK0gWXSw4upr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pipe.config import load_config\n",
        "from pipe.signif import run_significance\n",
        "\n",
        "cfg = load_config()  # default.yaml + (optional) local.yaml\n",
        "summary = run_significance(cfg, use_prepared_edges=True, eps=1e-9)\n",
        "\n",
        "print(\"[OK] Signifikanz-CSV(s) →\", cfg[\"outputs\"][\"signif_dir\"])\n",
        "summary.head()"
      ],
      "metadata": {
        "id": "JtmPHMRP5rbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pipe.config import load_config\n",
        "from pipe.signif_debug import run_signif_debug\n",
        "\n",
        "cfg = load_config()\n",
        "summary, outliers, notes = run_signif_debug(cfg, topk=10)\n",
        "print(notes)\n",
        "print(\"[OUT] z_debug_summary.csv  →\", cfg[\"outputs\"][\"signif_dir\"])\n",
        "print(\"[OUT] z_debug_outliers.csv →\", cfg[\"outputs\"][\"signif_dir\"])\n",
        "summary.head(), outliers.head()\n"
      ],
      "metadata": {
        "id": "2AhvOHoY7SJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pipe.config import load_config\n",
        "from pipe.leader_select import build_leaders_by_degree\n",
        "\n",
        "cfg = load_config()\n",
        "leaders = build_leaders_by_degree(cfg, use_prepared_edges=True)\n",
        "\n",
        "print(\"[OK] leaders.json & leaders_flat.csv →\", cfg[\"outputs\"][\"org_dir\"])\n",
        "print(\"Execs (n):\", len(leaders[\"execs\"]))\n",
        "for lv in leaders[\"levels\"]:\n",
        "    print(f\"{lv}: {len(leaders['levels'][lv])} Leader-Einträge\")\n"
      ],
      "metadata": {
        "id": "SsppBSja8IO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "from pipe.config import load_config\n",
        "from pipe.io import load_levels\n",
        "from pipe.orgchart_html import build_org_html\n",
        "\n",
        "# optional:\n",
        "try:\n",
        "    from pipe.persons import ensure_persons\n",
        "except ImportError:\n",
        "    ensure_persons = lambda cfg: pd.DataFrame()\n",
        "\n",
        "cfg = load_config()\n",
        "\n",
        "# Daten\n",
        "H = load_levels(cfg[\"paths\"][\"levels\"])\n",
        "P = ensure_persons(cfg)\n",
        "\n",
        "# Auswahl & Leader\n",
        "sel_path  = Path(cfg[\"outputs\"][\"org_dir\"]) / \"selection_dynamic.json\"\n",
        "lead_path = Path(cfg[\"outputs\"][\"org_dir\"]) / \"leaders.json\"\n",
        "selection = json.loads(sel_path.read_text())\n",
        "leaders   = json.loads(lead_path.read_text())\n",
        "\n",
        "# normalize cluster keys (string→int)\n",
        "leaders[\"levels\"] = {\n",
        "    lv: {int(k): v for k, v in (mp or {}).items()} for lv, mp in (leaders.get(\"levels\", {}) or {}).items()\n",
        "}\n",
        "\n",
        "# (optional) Z-Scores pro Level einsammeln\n",
        "z_by_level = {}\n",
        "signif_dir = Path(cfg[\"outputs\"][\"signif_dir\"])\n",
        "if signif_dir.exists():\n",
        "    for p in signif_dir.glob(\"cluster_significance_level_*.csv\"):\n",
        "        df = pd.read_csv(p)\n",
        "        if {\"cluster_id\", \"z_score\"}.issubset(df.columns):\n",
        "            lvl = p.stem.replace(\"cluster_significance_\", \"\")\n",
        "            z_by_level[lvl] = dict(zip(df[\"cluster_id\"].astype(int), df[\"z_score\"].astype(float)))\n",
        "\n",
        "# HTML erzeugen\n",
        "out_html = Path(cfg[\"outputs\"][\"org_dir\"]) / \"organigram_interaktiv.html\"\n",
        "path = build_org_html(\n",
        "    selection, leaders, H, persons=P,\n",
        "    out_html=str(out_html),\n",
        "    max_depth=min(3, len(selection.get(\"levels\", []))),\n",
        "    z_by_level=z_by_level,\n",
        "    topics_by_level={},   # kann später gefüllt werden\n",
        "    physics=False,\n",
        "    label_template=\"{level}:{cid} • {leader} • n={n} • z={z:.1f} • k_in={deg_in} • k={deg_global}\",\n",
        "    label_size=18\n",
        ")\n",
        "print(\"[OK] interaktive HTML:\", path)"
      ],
      "metadata": {
        "id": "CKM1mEav_kzG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}